{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "everyday-sudan",
   "metadata": {},
   "source": [
    "# Chatbot for a basic website \n",
    "\n",
    "## Based on Neural Network and made using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "legal-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-workstation",
   "metadata": {},
   "source": [
    "## Using NLTK for tokenizing, stemming and creating Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ultimate-cambridge",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anshu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## defining methods to \n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer=PorterStemmer()\n",
    "nltk.download('punkt')\n",
    "def tokenize(sentence):\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "def stem(word):\n",
    "    return stemmer.stem(word.lower())\n",
    "\n",
    "\n",
    "def bag_of_words(tokenized_sentence, all_words):\n",
    "    tokenized_sentence =[stem(w) for w in tokenized_sentence]\n",
    "    bag= np.zeros(len(all_words), dtype=np.float32)\n",
    "    for idx, w in enumerate(all_words):\n",
    "        if w in tokenized_sentence:\n",
    "            bag[idx]=1;\n",
    "            \n",
    "    return bag\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-forwarding",
   "metadata": {},
   "source": [
    "### Checking that the functions work fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fiscal-builder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whats up there buddy ?\n",
      "['whats', 'up', 'there', 'buddy', '?']\n"
     ]
    }
   ],
   "source": [
    "a=\"whats up there buddy ?\"\n",
    "\n",
    "print(a)\n",
    "print(tokenize(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "extraordinary-caribbean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['univers', 'go', 'reader', 'energ']\n"
     ]
    }
   ],
   "source": [
    "words=['Universe','going','reader','energize']\n",
    "\n",
    "stemmed_word=[stem(w) for w in words]\n",
    "print(stemmed_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-worry",
   "metadata": {},
   "source": [
    "## Importing the file to train on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adapted-adult",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'hey', 'how', 'are', 'you', 'is', 'anyon', 'there', 'hello', 'good', 'day', 'bye', 'see', 'you', 'later', 'goodby', 'thank', 'thank', 'you', 'that', \"'s\", 'help', 'thank', \"'s\", 'a', 'lot', 'which', 'item', 'do', 'you', 'have', 'what', 'kind', 'of', 'item', 'are', 'there', 'what', 'do', 'you', 'sell', 'do', 'you', 'take', 'credit', 'card', 'do', 'you', 'accept', 'mastercard', 'can', 'i', 'pay', 'with', 'paypal', 'are', 'you', 'cash', 'onli', 'how', 'long', 'doe', 'deliveri', 'take', 'how', 'long', 'doe', 'ship', 'take', 'when', 'do', 'i', 'get', 'my', 'deliveri', 'how', 'do', 'i', 'navig', 'your', 'websit', 'how', 'do', 'i', 'use', 'thi', 'websit', 'i', 'do', 'not', 'underst', 'how', 'to', 'oper', 'thi', 'websit']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('intents.json','r') as f:\n",
    "    intents=json.load(f)\n",
    "    \n",
    "all_words=[]\n",
    "tags=[]\n",
    "xy=[]\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    tag=intent['tag']\n",
    "    tags.append(tag)\n",
    "    \n",
    "    for pattern in intent['patterns']:\n",
    "        w = tokenize(pattern)\n",
    "        all_words.extend(w)\n",
    "        xy.append((w,tag))\n",
    "        \n",
    "ignore_words=['?','!','.',',']\n",
    "all_words=[w for w in all_words if w not in ignore_words]\n",
    "all_words=[stem(w) for w in all_words]\n",
    "#print(all_words)\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "outer-portfolio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(['Hi'], 'greeting'),\n",
       "  (['Hey'], 'greeting'),\n",
       "  (['How', 'are', 'you'], 'greeting'),\n",
       "  (['Is', 'anyone', 'there', '?'], 'greeting'),\n",
       "  (['Hello'], 'greeting'),\n",
       "  (['Good', 'day'], 'greeting'),\n",
       "  (['Bye'], 'goodbye'),\n",
       "  (['See', 'you', 'later'], 'goodbye'),\n",
       "  (['Goodbye'], 'goodbye'),\n",
       "  (['Thanks'], 'thanks'),\n",
       "  (['Thank', 'you'], 'thanks'),\n",
       "  (['That', \"'s\", 'helpful'], 'thanks'),\n",
       "  (['Thank', \"'s\", 'a', 'lot', '!'], 'thanks'),\n",
       "  (['Which', 'items', 'do', 'you', 'have', '?'], 'items'),\n",
       "  (['What', 'kinds', 'of', 'items', 'are', 'there', '?'], 'items'),\n",
       "  (['What', 'do', 'you', 'sell', '?'], 'items'),\n",
       "  (['Do', 'you', 'take', 'credit', 'cards', '?'], 'payments'),\n",
       "  (['Do', 'you', 'accept', 'Mastercard', '?'], 'payments'),\n",
       "  (['Can', 'I', 'pay', 'with', 'Paypal', '?'], 'payments'),\n",
       "  (['Are', 'you', 'cash', 'only', '?'], 'payments'),\n",
       "  (['How', 'long', 'does', 'delivery', 'take', '?'], 'delivery'),\n",
       "  (['How', 'long', 'does', 'shipping', 'take', '?'], 'delivery'),\n",
       "  (['When', 'do', 'I', 'get', 'my', 'delivery', '?'], 'delivery'),\n",
       "  (['How', 'do', 'I', 'navigate', 'your', 'website', '?'], 'services'),\n",
       "  (['How', 'do', 'I', 'use', 'this', 'website', '?'], 'services'),\n",
       "  (['I',\n",
       "    'do',\n",
       "    'not',\n",
       "    'understant',\n",
       "    'how',\n",
       "    'to',\n",
       "    'operate',\n",
       "    'this',\n",
       "    'website',\n",
       "    '.'],\n",
       "   'services')],)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-rabbit",
   "metadata": {},
   "source": [
    "### Sorting words, tags and keeping unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "vocational-arabic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'s\", 'a', 'accept', 'anyon', 'are', 'bye', 'can', 'card', 'cash', 'credit', 'day', 'deliveri', 'do', 'doe', 'get', 'good', 'goodby', 'have', 'hello', 'help', 'hey', 'hi', 'how', 'i', 'is', 'item', 'kind', 'later', 'long', 'lot', 'mastercard', 'my', 'navig', 'not', 'of', 'onli', 'oper', 'pay', 'paypal', 'see', 'sell', 'ship', 'take', 'thank', 'that', 'there', 'thi', 'to', 'underst', 'use', 'websit', 'what', 'when', 'which', 'with', 'you', 'your'] ['delivery', 'goodbye', 'greeting', 'items', 'payments', 'services', 'thanks']\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(all_words))\n",
    "tags=sorted(set(tags))\n",
    "print(all_words,tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-smile",
   "metadata": {},
   "source": [
    "### Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "consecutive-glenn",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create training data\n",
    "\n",
    "X_train=[]\n",
    "Y_train=[]\n",
    "\n",
    "for (sentence, tag) in xy:\n",
    "    bag=bag_of_words(sentence,all_words)\n",
    "    X_train.append(bag)\n",
    "    \n",
    "    label=tags.index(tag)\n",
    "    Y_train.append(label)\n",
    "    \n",
    "X_train=np.array(X_train)\n",
    "Y_train=np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "precise-overall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 2, 2, 2, 2, 2, 1, 1, 1, 6, 6, 6, 6, 3, 3, 3, 4, 4, 4, 4, 0, 0,\n",
       "        0, 5, 5, 5]),\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train,X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "generic-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "secure-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data=X_train\n",
    "        self.y_data=Y_train\n",
    "        \n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return self.x_data[index],self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "batch_size=8\n",
    "\n",
    "dataset=ChatDataset()\n",
    "train_loader=DataLoader(dataset=dataset, batch_size=batch_size,shuffle=True,num_workers=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-stylus",
   "metadata": {},
   "source": [
    "### Neural Network for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "practical-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        # no activation and no softmax at the end we'll use crossentropy loss\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-invention",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "normal-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=len(all_words)\n",
    "hidden_size=8\n",
    "output_size=len(tags)\n",
    "lr=0.001\n",
    "n_epochs=5\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model=NeuralNet(input_size,hidden_size,output_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-bidding",
   "metadata": {},
   "source": [
    "### Criterion and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fatty-bonus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of NeuralNet(\n",
      "  (l1): Linear(in_features=57, out_features=8, bias=True)\n",
      "  (l2): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (l3): Linear(in_features=8, out_features=7, bias=True)\n",
      "  (relu): ReLU()\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "criterion =nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-aurora",
   "metadata": {},
   "source": [
    "### Training the Model over the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "portuguese-benchmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=1.778104305267334\n",
      "loss=0.0612490214407444\n",
      "loss=0.11723238229751587\n",
      "loss=0.01758071780204773\n",
      "loss=0.00626613013446331\n",
      "loss=0.007749732118099928\n",
      "loss=0.00313715566881001\n",
      "loss=0.00148933962918818\n",
      "loss=0.00022807363711763173\n",
      "loss=0.0010217576054856181\n"
     ]
    }
   ],
   "source": [
    "##training loop\n",
    "n_epochs=1000\n",
    "for epoch in range(n_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words=words.to(device)\n",
    "        labels=labels.to(device)\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        #forward pass\n",
    "        \n",
    "        outputs=model(words)\n",
    "        loss=criterion(outputs,labels)\n",
    "        \n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #optimizer\n",
    "        optimizer.step()\n",
    "    if (epoch+1)%100 ==0:   \n",
    "        print(f'loss={loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-desktop",
   "metadata": {},
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "orange-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= {\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"input_size\":input_size,\n",
    "    \"output_size\":output_size,\n",
    "    \"hidden_size\":hidden_size,\n",
    "    \"all_words\" : all_words,\n",
    "    \"tags\":tags\n",
    "}\n",
    "\n",
    "FILE='data.path'\n",
    "\n",
    "torch.save(data,FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-allowance",
   "metadata": {},
   "source": [
    "### Using the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "reserved-thirty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's chat! (type 'quit' to exit)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "with open('intents.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "FILE = \"data.path\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-breed",
   "metadata": {},
   "source": [
    "### The ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "seven-torture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sober:Type 'Quit' to Exit\n",
      "You: Hey\n",
      "Sober: Hi there, how can I help?\n",
      "You: I dont understand how to use this website\n",
      "Sober: Search the product you want and select the best deal available.\n",
      "You: how\n",
      "Sober: Pardon? I did not understand...\n",
      "You: How long delivery takes ?\n",
      "Sober: Instant shippings! Really.\n",
      "You: bye\n",
      "Sober: Bye! Come back again soon.\n",
      "You: quit\n"
     ]
    }
   ],
   "source": [
    "bot_name = \"Sober\"\n",
    "print(f\"{bot_name}:Type 'Quit' to Exit\")\n",
    "\n",
    "while True:\n",
    "    # sentence = \"do you use credit cards?\"\n",
    "    sentence = input(\"You: \")\n",
    "    if sentence == \"quit\":\n",
    "        break\n",
    "\n",
    "    sentence = tokenize(sentence)\n",
    "    X = bag_of_words(sentence, all_words)\n",
    "    X = X.reshape(1, X.shape[0])\n",
    "    X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    output = model(X)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    tag = tags[predicted.item()]\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "    if prob.item() > 0.85:\n",
    "        for intent in intents['intents']:\n",
    "            if tag == intent[\"tag\"]:\n",
    "                print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "    else:\n",
    "        print(f\"{bot_name}: Pardon? I did not understand...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-horror",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
